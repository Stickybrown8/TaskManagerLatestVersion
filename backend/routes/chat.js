/*
 * ROUTE D'ASSISTANT DE PROGRAMMATION IA - backend/routes/chat.js
 *
 * Explication simple:
 * Ce fichier contient le code pour un assistant IA qui t'aide √† programmer.
 * Tu peux lui poser des questions, montrer des images de code ou des erreurs,
 * et il te r√©pond intelligemment en cherchant dans la base de connaissances du projet.
 * C'est comme un tuteur personnel qui connait tout ton code!
 *
 * Explication technique:
 * Route Express.js qui int√®gre l'API OpenAI (GPT-4/GPT-4o) avec une recherche s√©mantique
 * via MongoDB Atlas Vector Search pour offrir des r√©ponses contextuelles.
 * Supporte l'analyse de texte, d'images (vision) et le d√©bogage d'erreurs.
 *
 * O√π ce fichier est utilis√©:
 * Appel√© par le frontend quand l'utilisateur interagit avec l'assistant IA
 * dans l'interface de chat de l'application.
 *
 * Connexions avec d'autres fichiers:
 * - Importe les variables d'environnement depuis .env
 * - Se connecte √† la base MongoDB pour la recherche vectorielle
 * - Int√®gre l'API OpenAI pour le traitement du langage et des images
 * - Lit le fichier README.md du projet pour fournir du contexte
 * - Appel√© par les composants frontend de chat/assistant
 */

// === D√©but : Configuration de l'environnement et importation des d√©pendances ===
// Explication simple : On pr√©pare tous les outils dont notre assistant a besoin pour fonctionner, comme un bricoleur qui sort ses outils de sa bo√Æte.
// Explication technique : Importation des modules n√©cessaires - dotenv pour les variables d'environnement, express pour le routage, fs/path pour la manipulation de fichiers et les biblioth√®ques d'IA pour OpenAI et la recherche vectorielle.
require('dotenv').config();             // Charge .env

const express = require('express');
const fs = require('fs');
const path = require('path');
const router = express.Router();

const { MongoClient } = require('mongodb');
const { OpenAI } = require('openai');
const { MongoDBAtlasVectorSearch } = require('@langchain/mongodb');
const { OpenAIEmbeddings } = require('@langchain/openai');
// === Fin : Configuration de l'environnement et importation des d√©pendances ===

// === D√©but : Initialisation des clients OpenAI et MongoDB ===
// Explication simple : On connecte notre assistant √† son cerveau (OpenAI) et √† sa biblioth√®que de connaissances (MongoDB) pour qu'il puisse apprendre et r√©pondre.
// Explication technique : Configuration du client OpenAI avec l'API key et initialisation asynchrone du vector store MongoDB Atlas pour la recherche s√©mantique, avec gestion des erreurs.
// 1) Initialise OpenAI
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// 2) Initialise et connecte le vectorStore
let vectorStore;
(async function initVectorStore() {
  try {
    const client = new MongoClient(process.env.MONGO_URI);
    await client.connect();
    console.log('‚úÖ MongoDB client connect√©');

    const db = client.db('task_manager');
    const coll = db.collection('code_chunks');

    const embedder = new OpenAIEmbeddings({ openAIApiKey: process.env.OPENAI_API_KEY });
    vectorStore = new MongoDBAtlasVectorSearch(embedder, {
      collection: coll,
      indexName: 'vector_index',
      textKey: 'pageContent',
      embeddingKey: 'embedding',
      primaryKey: '_id',
    });

    console.log('‚úÖ Vector Store pr√™t');
  } catch (err) {
    console.error('‚ùå Erreur initVectorStore:', err);
  }
})();
// === Fin : Initialisation des clients OpenAI et MongoDB ===

// === D√©but : Fonction d'analyse des logs d'erreur ===
// Explication simple : Cette fonction est comme un d√©tective qui analyse un message d'erreur pour trouver des indices importants: quel type d'erreur, dans quel fichier et √† quelle ligne.
// Explication technique : Fonction utilitaire qui extrait les m√©tadonn√©es d'un log d'erreur via des expressions r√©guli√®res, identifiant le type d'erreur, le fichier source, le num√©ro de ligne et le code probl√©matique.
// NOUVELLE FONCTION: Extraire les infos d'erreur pour mieux cibler la recherche
function extractErrorInfo(errorLog) {
  const result = {
    errorType: '',
    fileName: '',
    lineNumber: null,
    errorMessage: '',
    exactError: ''
  };
  
  // Rechercher les erreurs TypeScript (tr√®s courant avec Netlify)
  const tsErrorMatch = errorLog.match(/TS(\d+):\s*(.*)/);
  if (tsErrorMatch) {
    result.errorType = `TypeScript Error TS${tsErrorMatch[1]}`;
    result.errorMessage = tsErrorMatch[2];
    result.exactError = tsErrorMatch[0];
  }
  
  // Rechercher les r√©f√©rences de fichiers et lignes
  const fileLineMatch = errorLog.match(/(\S+\.[a-zA-Z0-9]+):(\d+)(?::(\d+))?/);
  if (fileLineMatch) {
    result.fileName = fileLineMatch[1];
    result.lineNumber = parseInt(fileLineMatch[2]);
  }
  
  // Rechercher la ligne de code probl√©matique
  const codeLineMatch = errorLog.match(/>([^>]*\n[^>]*\^+[^>]*)/);
  if (codeLineMatch) {
    result.codeLine = codeLineMatch[1].trim();
  }
  
  return result;
}
// === Fin : Fonction d'analyse des logs d'erreur ===

// === D√©but : Fonction de pr√©paration des images pour l'API Vision ===
// Explication simple : Cette fonction v√©rifie si une image est dans le bon format pour √™tre envoy√©e au "robot qui voit". Si l'image n'est pas au bon format, elle essaie de la corriger.
// Explication technique : Utilitaire qui valide et normalise les images fournies, supportant les formats data URL, les URLs HTTP/HTTPS, et rejetant les formats non reconnus, pour les rendre compatibles avec l'API Vision de GPT-4.
// Fonction pour traiter les data URLs d'images (identique)
function prepareImageForGPT4Vision(dataURL) {
  if (!dataURL) return null;
  
  // V√©rifier si c'est d√©j√† une data URL correcte
  if (dataURL.startsWith('data:image/')) {
    // Nous utiliserons la data URL telle quelle
    console.log('üì∏ Image au format data URL d√©tect√©e');
    return dataURL;
  }
  
  // Si c'est une URL HTTP, la retourner directement
  if (dataURL.startsWith('http://') || dataURL.startsWith('https://')) {
    console.log('üì∏ Image au format URL Web d√©tect√©e');
    return dataURL;
  }
  
  // Si ce n'est ni une data URL ni une URL Web, c'est probablement un format non support√©
  console.warn('‚ö†Ô∏è Format d\'image non reconnu');
  return null;
}
// === Fin : Fonction de pr√©paration des images pour l'API Vision ===

// === D√©but : Route principale de l'API chat ===
// Explication simple : Ce gros bloc g√®re tout ce qui se passe quand tu envoies un message √† l'assistant. Il re√ßoit ta question, cherche des informations sur ton code, les envoie √† l'IA et te renvoie sa r√©ponse.
// Explication technique : Endpoint POST qui orchestre tout le processus de traitement des requ√™tes utilisateur: analyse du message, recherche de contexte, extraction d'informations d'erreur si n√©cessaire, pr√©paration des messages pour OpenAI et envoi de la r√©ponse format√©e.
// Route /api/chat
router.post('/', async (req, res) => {
  try {
    console.log("===== D√âBUT TRAITEMENT REQU√äTE CHAT =====");
    const { 
      message, 
      history = [], 
      imageData, 
      modelChoice = 'gpt-4.1',
      errorLog = null,           // NOUVEAU: Pour passer un log d'erreur
      isDeploymentError = false  // NOUVEAU: Mode analyse d'erreur
    } = req.body;
    
    console.log(`üì© Requ√™te re√ßue: ${message && message.length} caract√®res, mod√®le: ${modelChoice}`);
    console.log(`üìö ${history.length} messages dans l'historique`);
    console.log(`üîç Mode analyse d'erreur: ${isDeploymentError}`);
    
    // V√©rification et pr√©paration de l'image (identique)
    const hasImage = !!imageData;
    console.log(`üñºÔ∏è Image pr√©sente: ${hasImage}`, hasImage ? `(~${Math.round(imageData.length / 1024)}KB)` : '');
    
    let processedImageData = null;
    if (hasImage) {
      processedImageData = prepareImageForGPT4Vision(imageData);
      console.log(`üñºÔ∏è Image trait√©e: ${!!processedImageData}`);
    }
    
    // V√©rifier si le vectorStore est pr√™t
    if (!vectorStore) {
      return res.status(503).json({ error: 'Vector Store non pr√™t, patientez quelques instants' });
    }

    // === D√©but : Recherche contextuelle de documents pertinents ===
    // Explication simple : Ici, l'assistant cherche dans sa biblioth√®que de connaissances des informations qui pourront l'aider √† r√©pondre √† ta question, comme un d√©tective qui cherche des indices.
    // Explication technique : Bloc de recherche s√©mantique qui utilise MongoDB Atlas Vector Search pour r√©cup√©rer des documents pertinents, avec une strat√©gie diff√©rente selon qu'il s'agit d'une analyse d'erreur ou d'une requ√™te normale.
    // 3) Recherche des chunks pertinents - MODIFI√â POUR LES ERREURS
    let docs = [];
    let context = "";
    
    if (isDeploymentError && errorLog) {
      // Mode analyse d'erreur: extraire les infos pour cibler la recherche
      const errorInfo = extractErrorInfo(errorLog);
      console.log("üìä Infos d'erreur extraites:", errorInfo);
      
      // Construire une requ√™te cibl√©e
      const targetedQuery = `${errorInfo.errorType} ${errorInfo.errorMessage} ${errorInfo.fileName} ligne ${errorInfo.lineNumber}`;
      console.log(`üîé Requ√™te cibl√©e: ${targetedQuery}`);
      
      // Recherche sp√©cifique avec plus de r√©sultats pour le contexte d'erreur
      docs = await vectorStore.similaritySearch(targetedQuery, 8);
      
      // Si on a un nom de fichier, tenter de trouver des chunks contenant ce fichier sp√©cifique
      if (errorInfo.fileName) {
        const fileNameQuery = `filename:${errorInfo.fileName}`;
        const fileSpecificDocs = await vectorStore.similaritySearch(fileNameQuery, 3);
        
        // Combiner les r√©sultats en √©liminant les doublons
        const allDocs = [...docs, ...fileSpecificDocs];
        const uniqueDocs = allDocs.filter((doc, index, self) => 
          index === self.findIndex(d => d.pageContent === doc.pageContent)
        );
        
        docs = uniqueDocs.slice(0, 10); // Limiter √† 10 r√©sultats max
      }
    } else {
      // Mode normal: recherche bas√©e sur le message
      docs = await vectorStore.similaritySearch(message, 5);
    }
    
    // Formatter le contexte
    context = docs.map(d => {
      const source = d.metadata?.source ? `Source: ${d.metadata.source}` : '';
      return `${source}\n${d.pageContent}\n---\n`;
    }).join('\n');
    // === Fin : Recherche contextuelle de documents pertinents ===

    // === D√©but : Chargement des informations du projet ===
    // Explication simple : L'assistant essaie de comprendre ton projet en lisant son mode d'emploi (README), comme un nouvel √©l√®ve qui lit le manuel de classe avant de r√©pondre aux questions.
    // Explication technique : Lecture synchrone du fichier README.md du projet pour enrichir le contexte global de la conversation, avec gestion des erreurs si le fichier est inaccessible.
    // 4) Charger le README.md (identique)
    const readmePath = path.resolve(__dirname, '../../README.md');
    let readmeContent = '';
    try {
      readmeContent = fs.readFileSync(readmePath, 'utf8');
    } catch (err) {
      console.warn('‚ö†Ô∏è README non trouv√© ou illisible √†', readmePath, err.message);
    }
    // === Fin : Chargement des informations du projet ===

    // === D√©but : Pr√©paration des messages pour l'IA ===
    // Explication simple : Ici, on pr√©pare tout ce que l'assistant doit savoir avant de r√©pondre - sa mission, les informations du projet, l'historique de votre conversation et ta question actuelle.
    // Explication technique : Construction du tableau de messages pour l'API OpenAI, incluant le message syst√®me (instructions), le contexte (README, recherche vectorielle), l'historique de conversation et le message utilisateur courant.
    // 5) Pr√©pare les messages - MODIFI√â POUR LES ERREURS
    const messages = [];

    // Instruction syst√®me - adapt√©e pour les erreurs de d√©ploiement
    if (isDeploymentError) {
      messages.push({
        role: 'system',
        content: `Tu es un expert en d√©ploiement et debugging de code. Tu vas analyser une erreur de d√©ploiement Netlify.
        
INSTRUCTIONS IMPORTANTES:
1. Donne le fichier et la ligne exacte o√π se trouve l'erreur
2. Explique clairement la cause de l'erreur en termes simples
3. Propose une solution COMPL√àTE avec le code corrig√©
4. Indique TOUJOURS les √©tapes pr√©cises √† suivre pour corriger l'erreur

Format pour ta r√©ponse:
1. Identification pr√©cise de l'erreur: [Type d'erreur et explication]
2. Fichier concern√©: [chemin/vers/le/fichier:ligne]
3. Cause fondamentale: [explication pour d√©butant]
4. Solution propos√©e: [code complet pour corriger]
5. √âtapes pour appliquer la correction: [√©tapes num√©rot√©es]

N'oublie pas: Je suis d√©butant, sois tr√®s clair et pr√©cis!`
      });
      
      // Ajouter le log d'erreur en contexte prioritaire
      if (errorLog) {
        messages.push({
          role: 'system',
          content: `LOG D'ERREUR NETLIFY:\n${errorLog}`
        });
      }
    } else {
      // Mode normal
      messages.push({
        role: 'system',
        content: hasImage 
          ? 'Tu es un assistant qui aide √† analyser des images et du code. Examine attentivement l\'image et r√©ponds aux questions de l\'utilisateur. D√©cris exactement ce que tu vois dans l\'image.'
          : 'Tu es un assistant qui explique comme √† un enfant de 10 ans. Pour chaque erreur, indique toujours le fichier et la ligne. Explique pas √† pas chaque commande et fournis le code complet dans un bloc markdown avec le chemin du fichier. Utilise un langage simple et bienveillant.'
      });
    }

    // Contexte global du README
    if (readmeContent) {
      messages.push({
        role: 'system',
        content: `CONTEXTE GLOBAL DU PROJET (README.md) :\n${readmeContent}`
      });
    }

    // Contexte des extraits pertinents
    messages.push({ role: 'system', content: `Extraits pertinents :\n${context}` });

    // Ajouter l'historique des messages (identique)
    if (Array.isArray(history)) {
      const formattedHistory = history.map(msg => ({
        role: msg.role,
        content: msg.content
      }));
      messages.push(...formattedHistory);
    }
    
    // Message de l'utilisateur
    if (hasImage && processedImageData && modelChoice === 'gpt-4o') {
      // Format pour message avec image (identique)
      console.log("üì∏ Ajout d'une image au message avec GPT-4o Vision");
      messages.push({
        role: 'user',
        content: [
          { type: "text", text: message || "Que vois-tu dans cette image?" },
          {
            type: "image_url",
            image_url: { 
              url: processedImageData,
              detail: "high" // Demander une analyse d√©taill√©e
            }
          }
        ]
      });
    } else {
      // Message texte standard
      console.log("‚úâÔ∏è Ajout d'un message texte uniquement");
      
      // Si mode erreur et pas de message sp√©cifique, utiliser une demande par d√©faut
      const userMessage = isDeploymentError && !message.trim() 
        ? "Analyse cette erreur de d√©ploiement et dis-moi comment la corriger"
        : message;
        
      messages.push({ role: 'user', content: userMessage });
    }
    // === Fin : Pr√©paration des messages pour l'IA ===

    // === D√©but : Appel √† l'API OpenAI ===
    // Explication simple : C'est le moment o√π l'assistant envoie ta question au "grand cerveau" d'OpenAI et attend sa r√©ponse, comme quand tu poses une question difficile √† ton professeur.
    // Explication technique : Envoi de la requ√™te √† l'API OpenAI avec le mod√®le appropri√© (adaptable selon pr√©sence d'image ou mode d'erreur), param√®tres de g√©n√©ration ajust√©s et r√©cup√©ration de la r√©ponse.
    // D√©terminer le mod√®le √† utiliser - Pour les erreurs, toujours utiliser GPT-4
    const finalModel = hasImage ? 'gpt-4o' : (isDeploymentError ? 'gpt-4.1' : modelChoice);
    console.log(`üöÄ Envoi √† l'API OpenAI avec le mod√®le: ${finalModel}`);
    
    // 6) Appel √† l'API d'OpenAI avec le mod√®le s√©lectionn√©
    const completion = await openai.chat.completions.create({
      model: finalModel,
      messages,
      temperature: isDeploymentError ? 0.2 : 0.7, // Temp√©rature r√©duite pour les erreurs
      max_tokens: 2000
    });

    console.log(`‚úÖ R√©ponse re√ßue: ${completion.choices[0].message.content.length} caract√®res`);
    return res.json({ 
      reply: completion.choices[0].message.content,
      mode: isDeploymentError ? 'error_analysis' : 'chat'
    });
    // === Fin : Appel √† l'API OpenAI ===
  } catch (err) {
    // === D√©but : Gestion des erreurs ===
    // Explication simple : Si quelque chose ne fonctionne pas, ce bloc s'occupe de cr√©er un message d'erreur gentil pour t'expliquer ce qui s'est pass√©, comme quand un jeu vid√©o te dit pourquoi il a plant√©.
    // Explication technique : Bloc de gestion d'exception qui capture, journalise et formate les erreurs rencontr√©es pendant le traitement, avec une attention particuli√®re aux erreurs li√©es au traitement d'images.
    console.error('‚ùå Erreur chat d√©taill√©e:', err);
    
    // Pr√©parer un message d'erreur utilisateur
    let userErrorMessage = "Une erreur s'est produite lors du traitement de votre demande.";
    
    // V√©rifier si l'erreur est li√©e √† l'image
    if (err.message && (
        err.message.includes('image') || 
        err.message.includes('vision') || 
        err.message.includes('not supported')
    )) {
      userErrorMessage = "Erreur lors du traitement de l'image. Veuillez v√©rifier que l'image est valide et r√©essayer.";
    }
    
    return res.status(500).json({ 
      error: err.message,
      reply: userErrorMessage
    });
    // === Fin : Gestion des erreurs ===
  }
});
// === Fin : Route principale de l'API chat ===

// === D√©but : Exportation du routeur ===
// Explication simple : Cette ligne rend notre assistant disponible pour le reste de l'application, comme quand tu mets ton jouet dans un endroit o√π tes amis peuvent le trouver.
// Explication technique : Exportation du routeur Express configur√© pour permettre son montage dans l'application principale via le syst√®me de middleware Express.
module.exports = router;
// === Fin : Exportation du routeur ===