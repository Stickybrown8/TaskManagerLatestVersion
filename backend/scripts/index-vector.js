// === Ce fichier cr√©e une base de donn√©es intelligente de tout le code du projet pour pouvoir y faire des recherches === /workspaces/TaskManagerLatestVersion/backend/scripts/index-vector.js
// Explication simple : Ce script est comme un robot qui lit tous les fichiers du projet, les d√©coupe en petits morceaux, et leur attribue une "empreinte digitale" sp√©ciale pour pouvoir retrouver tr√®s rapidement n'importe quelle partie du code en posant une question en langage naturel.
// Explication technique : Script Node.js autonome qui indexe les fichiers du projet en chunks textuels, g√©n√®re des embeddings vectoriels via OpenAI, et les stocke dans une collection MongoDB avec recherche vectorielle.
// Utilis√© dans : Ex√©cut√© manuellement via la commande "node scripts/index-vector.js" pour initialiser ou mettre √† jour l'index vectoriel du code source.
// Connect√© √† : Base de donn√©es MongoDB (collection 'code_chunks'), API OpenAI pour la g√©n√©ration d'embeddings, et indirectement au syst√®me de chat qui utilisera cette base pour r√©pondre aux questions sur le code.

// filepath: /workspaces/TaskManagerLatestVersion/backend/scripts/index-vector.js
// backend/scripts/index-vector.js

// === D√©but : Importation des d√©pendances ===
// Explication simple : Ces lignes sont comme une liste d'outils sp√©ciaux que notre robot va utiliser pour faire son travail - un outil pour lire des fichiers, un autre pour les d√©couper, encore un autre pour cr√©er des "empreintes digitales", etc.
// Explication technique : Importation des biblioth√®ques Node.js n√©cessaires : configuration des variables d'environnement, manipulation de fichiers, recherche de fichiers par pattern, LangChain pour le traitement du texte et les embeddings, et MongoDB pour le stockage vectoriel.
require('dotenv/config');
const fs = require('fs');
const path = require('path');
const glob = require('glob');
const { RecursiveCharacterTextSplitter } = require('@langchain/textsplitters');
const { OpenAIEmbeddings } = require('@langchain/openai');
const { MongoDBAtlasVectorSearch } = require('@langchain/mongodb');
const { MongoClient } = require('mongodb');
// === Fin : Importation des d√©pendances ===

// === D√©but : Fonction principale d'indexation ===
// Explication simple : Cette fonction est comme le cerveau de notre robot qui organise √©tape par √©tape toutes les actions √† faire pour cr√©er notre base de donn√©es intelligente.
// Explication technique : Fonction asynchrone principale qui orchestre l'ensemble du processus d'indexation, de la connexion √† MongoDB jusqu'√† la g√©n√©ration et sauvegarde des embeddings vectoriels.
async function main() {
  // === D√©but : Connexion √† la base de donn√©es ===
  // Explication simple : Cette partie aide notre robot √† se connecter au grand entrep√¥t o√π il va ranger toutes les informations qu'il va collecter.
  // Explication technique : Initialisation de la connexion au client MongoDB en utilisant l'URI stock√©e dans les variables d'environnement, puis s√©lection de la base de donn√©es et de la collection sp√©cifique.
  // 1) Connexion au cluster MongoDB
  const client = new MongoClient(process.env.MONGO_URI);
  await client.connect();
  console.log('‚úÖ MongoDB client connect√© pour indexation');
  const db = client.db('task_manager');
  const collection = db.collection('code_chunks');
  // === Fin : Connexion √† la base de donn√©es ===

  // === D√©but : Recherche des fichiers √† indexer ===
  // Explication simple : Ici, notre robot parcourt tous les dossiers du projet pour trouver les fichiers int√©ressants √† lire, mais ignore les fichiers qui ne sont pas importants comme ceux dans les poubelles (node_modules).
  // Explication technique : D√©finition du r√©pertoire racine et utilisation de glob pour rechercher tous les fichiers avec les extensions sp√©cifi√©es, en excluant les r√©pertoires non pertinents via le pattern et les options ignore.
  // 2) R√©cup√©rer tous les fichiers pertinents - AM√âLIORATION: inclure frontend et plus de types de fichiers
  const rootDir = path.resolve(__dirname, '../..');  // Remonte √† la racine du projet
  const pattern = '**/*.@(ts|tsx|js|jsx|md|json|html|css)';  // Plus de types de fichiers
  
  // Inclure les fichiers du frontend et du backend, mais exclure les dossiers node_modules, build, etc.
  const files = glob.sync(pattern, {
    cwd: rootDir,
    ignore: ['**/node_modules/**', '**/build/**', '**/dist/**', '**/scripts/**']
  });

  console.log(`üîç ${files.length} fichiers trouv√©s √† indexer`);
  // === Fin : Recherche des fichiers √† indexer ===

  // === D√©but : D√©coupage des fichiers en chunks ===
  // Explication simple : Notre robot d√©coupe maintenant chaque fichier en petits morceaux faciles √† dig√©rer, comme quand tu coupes une grande pizza en parts pour mieux la manger.
  // Explication technique : Configuration du splitter LangChain pour d√©couper les textes avec une taille et un chevauchement optimis√©s, puis lecture et d√©coupage s√©quentiel de chaque fichier avec enrichissement des m√©tadonn√©es.
  // 3) Lire et chunker manuellement
  const splitter = new RecursiveCharacterTextSplitter({ 
    chunkSize: 1500,      // Taille r√©duite pour plus de pr√©cision
    chunkOverlap: 200
  });
  const docs = [];
  
  for (const filePath of files) {
    try {
      const fullPath = path.join(rootDir, filePath);
      console.log(`Indexation de: ${filePath}`);
      
      const content = fs.readFileSync(fullPath, 'utf8');
      const chunks = await splitter.splitText(content);
      
      chunks.forEach((text, index) => {
        // Enrichir les m√©tadonn√©es pour faciliter la recherche
        docs.push({ 
          pageContent: text, 
          metadata: { 
            source: filePath,
            chunk: index + 1,
            totalChunks: chunks.length,
            fileType: path.extname(filePath),
            isFrontend: filePath.includes('frontend/'),
            isBackend: filePath.includes('backend/'),
            isModel: filePath.includes('models/') || filePath.includes('schema'),
            lastModified: fs.statSync(fullPath).mtime.toISOString()
          }
        });
      });
    } catch (error) {
      console.error(`‚ö†Ô∏è Erreur lors de l'indexation de ${filePath}:`, error.message);
    }
  }
  // === Fin : D√©coupage des fichiers en chunks ===

  // === D√©but : G√©n√©ration des embeddings ===
  // Explication simple : Ici, notre robot cr√©e une "empreinte digitale" sp√©ciale pour chaque morceau de texte, qui permettra plus tard de retrouver les morceaux qui ressemblent √† ce qu'on cherche.
  // Explication technique : Initialisation du mod√®le d'embeddings OpenAI qui transformera les chunks textuels en vecteurs num√©riques, en utilisant la cl√© API fournie dans les variables d'environnement.
  // 4) G√©n√©rer les embeddings
  const embedder = new OpenAIEmbeddings({ openAIApiKey: process.env.OPENAI_API_KEY });
  // === Fin : G√©n√©ration des embeddings ===

  // === D√©but : Configuration du vector store ===
  // Explication simple : Cette partie pr√©pare le grand entrep√¥t sp√©cial o√π notre robot va ranger toutes les "empreintes digitales" des morceaux de texte.
  // Explication technique : Initialisation de l'int√©gration MongoDB Atlas Vector Search de LangChain avec la configuration des champs pour le stockage des embeddings et des m√©tadonn√©es.
  // 5) Initialiser le vector store (identique)
  const vectorStore = new MongoDBAtlasVectorSearch(embedder, {
    collection,
    indexName: 'vector_index',
    textKey: 'pageContent',
    embeddingKey: 'embedding',
    primaryKey: '_id',
  });
  // === Fin : Configuration du vector store ===

  // === D√©but : Sauvegarde des documents dans la base de donn√©es ===
  // Explication simple : Notre robot efface d'abord toutes les anciennes informations pour faire place nette, puis range soigneusement tous les nouveaux morceaux de texte avec leurs "empreintes digitales" dans l'entrep√¥t.
  // Explication technique : Suppression des anciens documents de la collection pour √©viter les duplications, puis ajout des nouveaux documents avec leurs embeddings g√©n√©r√©s par le mod√®le OpenAI.
  // 6) Ajouter les documents
  // D'abord, effacer l'ancienne indexation
  await collection.deleteMany({});
  console.log("‚úÖ Ancienne indexation supprim√©e");
  
  // Ajouter les nouveaux documents
  await vectorStore.addDocuments(docs);
  console.log(`‚úÖ Indexation termin√©e : ${docs.length} chunks enregistr√©s.`);
  // === Fin : Sauvegarde des documents dans la base de donn√©es ===

  // === D√©but : Fermeture de la connexion ===
  // Explication simple : Apr√®s avoir fini tout son travail, notre robot ferme proprement la porte de l'entrep√¥t pour s'assurer que tout est bien s√©curis√©.
  // Explication technique : Fermeture appropri√©e de la connexion MongoDB pour lib√©rer les ressources syst√®me et √©viter les fuites de connexion.
  // 7) Fermer la connexion
  await client.close();
  // === Fin : Fermeture de la connexion ===
}
// === Fin : Fonction principale d'indexation ===

// === D√©but : Ex√©cution du script ===
// Explication simple : Cette derni√®re partie est comme le bouton "D√©marrer" qui lance notre robot et le surveille pour s'assurer qu'il ne tombe pas en panne.
// Explication technique : Pattern d'auto-ex√©cution de la fonction principale avec gestion des erreurs, affichant les exceptions dans la console et terminant le processus avec un code d'erreur en cas d'√©chec.
main().catch(err => {
  console.error('‚ùå Erreur index-vector:', err);
  process.exit(1);
});
// === Fin : Ex√©cution du script ===